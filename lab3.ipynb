{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>bachelor's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>completed</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female</td>\n",
       "      <td>group B</td>\n",
       "      <td>master's degree</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male</td>\n",
       "      <td>group A</td>\n",
       "      <td>associate's degree</td>\n",
       "      <td>free/reduced</td>\n",
       "      <td>none</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>group C</td>\n",
       "      <td>some college</td>\n",
       "      <td>standard</td>\n",
       "      <td>none</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender race/ethnicity parental level of education         lunch  \\\n",
       "0  female        group B           bachelor's degree      standard   \n",
       "1  female        group C                some college      standard   \n",
       "2  female        group B             master's degree      standard   \n",
       "3    male        group A          associate's degree  free/reduced   \n",
       "4    male        group C                some college      standard   \n",
       "\n",
       "  test preparation course  math score  reading score  writing score  \n",
       "0                    none          72             72             74  \n",
       "1               completed          69             90             88  \n",
       "2                    none          90             95             93  \n",
       "3                    none          47             57             44  \n",
       "4                    none          76             78             75  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data():\n",
    "    path = 'data/StudentsPerformance.csv'\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "data = load_data()\n",
    "data.head()\n",
    "\n",
    "# data[\"test preparation course\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all features into numerical values so we can calculate R values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race/ethnicity</th>\n",
       "      <th>parental level of education</th>\n",
       "      <th>lunch</th>\n",
       "      <th>test preparation course</th>\n",
       "      <th>math score</th>\n",
       "      <th>reading score</th>\n",
       "      <th>writing score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>57</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>99</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>71</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>78</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gender  race/ethnicity  parental level of education  lunch  \\\n",
       "0         1            0.75                          0.8      0   \n",
       "1         1            0.50                          0.4      0   \n",
       "2         1            0.75                          1.0      0   \n",
       "3         0            1.00                          0.6      1   \n",
       "4         0            0.50                          0.4      0   \n",
       "..      ...             ...                          ...    ...   \n",
       "995       1            0.00                          1.0      0   \n",
       "996       0            0.50                          0.2      1   \n",
       "997       1            0.50                          0.2      1   \n",
       "998       1            0.25                          0.4      0   \n",
       "999       1            0.25                          0.4      1   \n",
       "\n",
       "     test preparation course  math score  reading score  writing score  \n",
       "0                          0          72             72             74  \n",
       "1                          1          69             90             88  \n",
       "2                          0          90             95             93  \n",
       "3                          0          47             57             44  \n",
       "4                          0          76             78             75  \n",
       "..                       ...         ...            ...            ...  \n",
       "995                        1          88             99             95  \n",
       "996                        0          62             55             55  \n",
       "997                        1          59             71             65  \n",
       "998                        1          68             78             77  \n",
       "999                        0          77             86             86  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert everything into numbers for calculating Pearson's R\n",
    "def genderToNum(row):\n",
    "    if (row[\"gender\"] == \"male\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def raceToNum(row):\n",
    "    if row[\"race/ethnicity\"][-1] == \"A\":\n",
    "        return 1\n",
    "    elif row[\"race/ethnicity\"][-1] == \"B\":\n",
    "        return 0.75\n",
    "    elif row[\"race/ethnicity\"][-1] == \"C\":\n",
    "        return 0.5\n",
    "    elif row[\"race/ethnicity\"][-1] == \"D\":\n",
    "        return 0.25\n",
    "    elif row[\"race/ethnicity\"][-1] == \"E\":\n",
    "        return 0\n",
    "\n",
    "def eduToNum(row):\n",
    "    if row[\"parental level of education\"] == \"some high school\":\n",
    "        return 0\n",
    "    elif row[\"parental level of education\"] == \"high school\":\n",
    "        return 0.2\n",
    "    elif row[\"parental level of education\"] == \"some college\":\n",
    "        return 0.4\n",
    "    elif row[\"parental level of education\"] == \"associate's degree\":\n",
    "        return 0.6\n",
    "    elif row[\"parental level of education\"] == \"bachelor's degree\":\n",
    "        return 0.8\n",
    "    elif row[\"parental level of education\"] == \"master's degree\":\n",
    "        return 1\n",
    "\n",
    "def lunchToNum(row):\n",
    "    if row[\"lunch\"] == \"standard\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def prepToNum(row):\n",
    "    if row[\"test preparation course\"] == \"none\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "        \n",
    "# def binIt(row):\n",
    "#     print(row[\"math score\"])\n",
    "#     if (row[\"math score\"] >= 70):\n",
    "#         return \"A\"\n",
    "#     elif (row[\"math score\"] >= 60):\n",
    "#         return \"B\"\n",
    "#     elif (row[\"math score\"] >= 50):\n",
    "#         return \"C\"\n",
    "#     elif (row[\"math score\"] >= 40):\n",
    "#         return \"D\"\n",
    "#     else:\n",
    "#         return \"F\"\n",
    "\n",
    "data[\"gender\"] = data.apply(genderToNum, axis=1)\n",
    "data[\"race/ethnicity\"] = data.apply(raceToNum, axis=1)\n",
    "data[\"parental level of education\"] = data.apply(eduToNum, axis=1)\n",
    "data[\"lunch\"] = data.apply(lunchToNum, axis=1)\n",
    "data[\"test preparation course\"] = data.apply(prepToNum, axis=1)\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find and order the Pearson's R values for each feature against each math/reading/writing score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features against Math scores\n",
      "reading score:  0.8175796636720544\n",
      "writing score:  0.8026420459498085\n",
      "lunch:  0.3508766455918607\n",
      "race/ethnicity:  0.21641544829808895\n",
      "test preparation course:  0.17770246930439465\n",
      "gender:  0.1679822381003558\n",
      "parental level of education:  0.15943181815735608\n",
      "\n",
      "\n",
      "Features against Reading scores\n",
      "writing score:  0.9545980771462478\n",
      "math score:  0.8175796636720544\n",
      "gender:  0.24431260787747192\n",
      "test preparation course:  0.24178043354875134\n",
      "lunch:  0.2295603216622811\n",
      "parental level of education:  0.1909082647642037\n",
      "race/ethnicity:  0.14525262214153506\n",
      "\n",
      "\n",
      "Features against Writing scores\n",
      "reading score:  0.9545980771462478\n",
      "math score:  0.8026420459498085\n",
      "test preparation course:  0.31294628448595596\n",
      "gender:  0.3012249355007125\n",
      "lunch:  0.2457686763842185\n",
      "parental level of education:  0.23671517332205244\n",
      "race/ethnicity:  0.16569051050724565\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import statistics\n",
    "\n",
    "def printDict(list):\n",
    "    for x in list:\n",
    "        print(\"{}:  {}\".format(x[0], x[1]))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Get the Pearson's R value between each feature and the different scores\n",
    "mathlist = {}\n",
    "for x in data.keys():\n",
    "    if x == \"math score\":\n",
    "        continue\n",
    "    mathlist[x] = abs(np.corrcoef(data[x], data[\"math score\"])[0][1])\n",
    "mathlist = sorted(mathlist.items(), key=lambda x:x[1], reverse=True)\n",
    "print(\"Features against Math scores\")\n",
    "printDict(mathlist)\n",
    "\n",
    "readinglist = {}\n",
    "for x in data.keys():\n",
    "    if x == \"reading score\":\n",
    "        continue\n",
    "    readinglist[x] = abs(np.corrcoef(data[x], data[\"reading score\"])[0][1])\n",
    "readinglist = sorted(readinglist.items(), key=lambda x:x[1], reverse=True)\n",
    "print(\"Features against Reading scores\")\n",
    "printDict(readinglist)\n",
    "\n",
    "writinglist = {}\n",
    "for x in data.keys():\n",
    "    if x == \"writing score\":\n",
    "        continue\n",
    "    writinglist[x] = abs(np.corrcoef(data[x], data[\"writing score\"])[0][1])\n",
    "writinglist = sorted(writinglist.items(), key=lambda x:x[1], reverse=True)\n",
    "print(\"Features against Writing scores\")\n",
    "printDict(writinglist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate Selection for top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  features        score\n",
      "6            writing score  2289.670475\n",
      "5            reading score  2136.299236\n",
      "3                    lunch   118.122326\n",
      "4  test preparation course    56.923719\n",
      "0                   gender    44.949761\n",
      "1           race/ethnicity    20.140823\n"
     ]
    }
   ],
   "source": [
    "# This was all essentially from the recommended webpage on feature selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "x = data.iloc[:,0:8]    # Select all columns for the selection process\n",
    "x = x.drop(columns=[\"math score\"])  # drop math score from the selection process as it is the target column\n",
    "y = data.iloc[:,-3] # The target column is the math score\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=6)\n",
    "fit = bestfeatures.fit(x,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(x.columns)\n",
    "featureScores = pd.concat([dfcolumns,dfscores], axis=1)\n",
    "featureScores.columns = ['features', 'score']\n",
    "print(featureScores.nlargest(6,'score'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, test preparation course is considered to have a stronger relationship to math score in the Univariate Selection than in the Pearson's R method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we are focussing on the maths score as the target attribute, we will create data sets of sizes 6, 4, and 2 of the features that correlate strongest with the maths score. The new data sets are written to separate csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the correlation scores from Univariate Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Top 2\n",
    "# writing, reading\n",
    "topTwo = data[[\"writing score\", \"reading score\"]]\n",
    "topTwo.to_csv('data/topTwoStudentPerf.csv', index=False)\n",
    "\n",
    "# Top 4\n",
    "# writing, reading, lunch, test prep\n",
    "topFour = data[[\"writing score\", \"writing score\", \"lunch\", \"test preparation course\"]]\n",
    "topFour.to_csv('data/topFourStudentPerf.csv', index=False)\n",
    "\n",
    "# Top 6\n",
    "# writing, reading, lunch, test prep, gender, ethnicity\n",
    "topSix = data[[\"writing score\", \"writing score\", \"lunch\", \"test preparation course\", \"gender\", \"race/ethnicity\"]]\n",
    "topSix.to_csv('data/topSixStudentPerf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use test/train split to separate the data and use linear regression as the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set predictions for topTwo:\n",
      "[69.3173217  69.51001596 47.21659852 60.64406104 73.62176902 68.19293535\n",
      " 67.61485258 43.49023397 80.52819454 36.35893119 48.95125065 67.13291503\n",
      " 76.22374722 75.80617569 55.44010464 42.43021365 48.72637338 84.44725334\n",
      " 53.51275825 74.74615537 61.89677562 56.91769649 73.2363805  57.78502256\n",
      " 45.8673349  61.89677562 53.93032978 45.25706912 67.06854901 84.02968181\n",
      " 80.75307181 80.62474357 87.08141455 59.74455196 55.6327989  76.06323597\n",
      " 93.15269701 55.60061589 73.8788293  69.99195351 62.37871317 54.12302404\n",
      " 68.3534466  74.29640083 78.40815389 58.26696011 84.67213061 60.64406104\n",
      " 79.56472325 60.8367553  38.06140031 59.74455196 50.01127098 54.76547283\n",
      " 41.9482761  70.85927958 53.51275825 60.61187803 71.69442263 62.34653016\n",
      " 65.17338563 75.16372689 53.89814677 71.40517934 64.53093683 65.36607989\n",
      " 68.61050688 53.0308207  67.13291503 50.87859704 68.44999563 79.17893092\n",
      " 69.51001596 66.68316049 54.09084103 62.31434715 68.25730138 67.13291503\n",
      " 31.79742359 74.93884962 74.71397236 83.61211029 69.06026142 80.36768329\n",
      " 63.27822225 72.30468841 63.88848803 83.0018445  73.62176902 59.96942923\n",
      " 72.52956568 62.82846771 62.7962847  55.21522737 71.88711689 69.92758749\n",
      " 47.82686431 70.98760781 62.53922442 61.67189835 49.07957889 82.6804182\n",
      " 61.25432682 46.47760069 70.21683078 76.03105296 69.28513869 61.89677562\n",
      " 60.16212349 83.09798972 68.41781262 72.59393171 34.88133934 55.56843288\n",
      " 69.06026142 72.11199416 77.73352208 74.42472906 77.54082782 59.5518577\n",
      " 55.21522737 79.27547996 87.08141455 77.09107328 70.15246476 62.57140743\n",
      " 70.15246476 74.07152356 65.62314017 61.47920409 62.37871317 63.27822225\n",
      " 25.21202056 77.34813357 83.35505001 57.62451131 60.90112132 62.60359044\n",
      " 69.70271022 77.99058236 70.60221929 66.29777198 49.59369945 56.88551348\n",
      " 79.66086847 39.12142063 43.71511124 44.5502543  79.95011176 80.75307181\n",
      " 47.34492675 70.60221929 92.51024822 66.29777198 76.41644148 75.32423814\n",
      " 51.90643436 56.72500224 58.17041108 50.17178222 90.32584155 55.79331015\n",
      " 53.57672046 86.63166001 61.57575313 77.05889027 76.06323597 63.21385623\n",
      " 54.73328982 58.26696011 58.65234863 66.52264924 38.89654336 65.84801744\n",
      " 79.98229478 67.29342628 64.94850836 52.83812644 92.02831067 70.85927958\n",
      " 59.74455196 81.01013209 64.08118229 63.4387335  80.33550028 63.66361077\n",
      " 90.35802456 93.15269701 74.29640083 42.20533638 48.66200736 58.00989983\n",
      " 49.17612792 77.28376754 57.78502256 59.10210317 84.02968181 76.60913573\n",
      " 68.03242411 64.98069137 72.94713721 75.61348143 54.3157183  88.59118941\n",
      " 68.48217865 57.75283955 76.4808075  82.48772394 74.07152356 65.01287438\n",
      " 55.82549316 84.70431362 60.80457229 61.76844739 36.80868573 72.33687143\n",
      " 81.23500936 63.18167321 57.75283955 72.33687143 75.09936087 64.49875382\n",
      " 56.08255344 72.81880898 62.34653016 73.8788293  83.83698756 56.50012497\n",
      " 43.74729426 63.40655048 50.13959921 67.10073202 50.87859704 51.29616857\n",
      " 57.97771682 65.3982629  69.3173217  60.1943065  66.04071169 62.37871317\n",
      " 78.40815389 51.97080038 79.72523449 58.00989983 76.64131875 68.48217865\n",
      " 59.32698043 88.36631214 63.27822225 82.93747848]\n",
      "Test set score: 0.61\n",
      "\n",
      "\n",
      "Test set predictions for topFour:\n",
      "[68.62054443 67.12054443 47.62054443 60.87054443 74.87054443 66.87054443\n",
      " 74.12054443 48.49554443 76.37054443 43.87054443 56.37054443 65.62054443\n",
      " 74.87054443 78.12054443 53.62054443 44.62054443 55.62054443 81.12054443\n",
      " 60.87054443 76.37054443 63.12054443 54.62054443 78.12054443 60.12054443\n",
      " 45.37054443 58.37054443 60.12054443 50.12054443 60.87054443 81.87054443\n",
      " 74.62054443 83.12054443 88.12054443 62.62054443 52.12054443 81.12054443\n",
      " 93.62054443 54.62054443 75.62054443 75.62054443 67.12054443 53.87054443\n",
      " 65.62054443 74.87054443 75.62054443 61.62054443 84.37054443 60.87054443\n",
      " 84.12054443 59.37054443 43.12054443 57.87054443 50.87054443 58.62054443\n",
      " 38.24554443 71.87054443 56.12054443 63.12054443 72.37054443 60.12054443\n",
      " 63.37054443 75.62054443 50.62054443 69.37054443 65.62054443 59.12054443\n",
      " 68.62054443 57.12054443 67.87054443 53.87054443 72.62054443 69.37054443\n",
      " 71.87054443 66.12054443 51.62054443 62.62054443 74.12054443 70.37054443\n",
      " 39.24554443 77.37054443 74.12054443 84.87054443 70.37054443 80.37054443\n",
      " 67.87054443 72.62054443 63.37054443 84.87054443 67.62054443 58.37054443\n",
      " 68.62054443 68.62054443 61.62054443 55.37054443 73.37054443 71.12054443\n",
      " 47.74554443 65.62054443 58.37054443 62.62054443 47.74554443 74.62054443\n",
      " 63.12054443 40.49554443 76.62054443 71.62054443 63.87054443 58.37054443\n",
      " 61.87054443 76.37054443 67.87054443 75.62054443 40.62054443 49.99554443\n",
      " 70.37054443 74.12054443 70.87054443 66.37054443 72.12054443 61.62054443\n",
      " 55.37054443 76.37054443 85.62054443 78.12054443 64.62054443 58.37054443\n",
      " 71.87054443 69.37054443 66.87054443 64.12054443 64.62054443 70.37054443\n",
      " 22.74554443 81.12054443 79.37054443 59.37054443 66.12054443 60.87054443\n",
      " 70.37054443 81.12054443 70.87054443 71.87054443 56.37054443 57.12054443\n",
      " 73.37054443 37.49554443 44.62054443 42.87054443 76.37054443 79.37054443\n",
      " 48.49554443 73.37054443 91.12054443 64.62054443 73.12054443 66.87054443\n",
      " 48.37054443 60.87054443 57.12054443 51.62054443 90.37054443 55.62054443\n",
      " 45.37054443 86.62054443 63.87054443 75.62054443 73.87054443 65.62054443\n",
      " 56.37054443 64.12054443 60.87054443 65.37054443 36.74554443 63.12054443\n",
      " 85.62054443 66.37054443 59.87054443 58.62054443 87.37054443 76.62054443\n",
      " 60.12054443 78.12054443 59.12054443 66.37054443 73.12054443 67.12054443\n",
      " 90.37054443 93.62054443 72.62054443 48.49554443 50.74554443 60.87054443\n",
      " 49.87054443 76.62054443 55.37054443 57.87054443 77.12054443 71.62054443\n",
      " 70.87054443 66.87054443 67.87054443 79.37054443 54.62054443 89.12054443\n",
      " 67.62054443 53.12054443 80.37054443 81.12054443 74.12054443 64.62054443\n",
      " 57.87054443 84.12054443 57.12054443 69.37054443 40.62054443 70.12054443\n",
      " 81.12054443 63.12054443 53.12054443 74.87054443 68.62054443 65.62054443\n",
      " 60.87054443 78.87054443 62.37054443 70.87054443 78.37054443 60.12054443\n",
      " 44.49554443 59.37054443 44.62054443 63.37054443 49.12054443 55.62054443\n",
      " 53.87054443 68.62054443 73.37054443 61.62054443 66.12054443 62.37054443\n",
      " 73.12054443 57.87054443 80.37054443 60.87054443 74.12054443 74.87054443\n",
      " 63.12054443 88.12054443 70.37054443 78.12054443]\n",
      "Test set score: 0.63\n",
      "\n",
      "\n",
      "Test set predictions for topSix:\n",
      "[65.65362905 76.28955005 51.00048755 69.47616075 69.83420585 72.34423755\n",
      " 68.88108085 50.63330005 85.56902145 45.73397325 46.81858085 60.10853265\n",
      " 81.93798755 87.07861255 44.40540765 35.0646746  58.40762685 76.20139335\n",
      " 50.85160946 82.13507866 54.58509515 48.39581655 72.81165765 64.15762685\n",
      " 50.28866075 65.73486255 64.15762685 50.38418935 55.20920585 77.16233085\n",
      " 71.61634515 92.48486255 84.42192196 53.63197015 44.20831655 90.92236255\n",
      " 93.71612905 56.99267505 69.48264335 71.65362905 74.50741075 59.98486255\n",
      " 70.01789116 69.38800405 74.29425405 52.22483085 81.34983085 55.20920585\n",
      " 80.89581655 53.29514335 43.9184563  63.92325185 42.74045585 63.10205005\n",
      " 40.339442   69.49737905 50.30987905 68.85205005 80.77214645 55.10675405\n",
      " 72.35897325 71.19961475 43.15273975 76.53173755 69.57168935 65.39111255\n",
      " 76.42928575 47.77170585 59.03822015 54.32257866 81.22616075 80.03084825\n",
      " 80.26522325 57.11634515 58.82683395 68.75741075 83.14803575 78.35116075\n",
      " 25.7131121  71.85853265 80.97705005 83.16144155 65.80117725 87.79825185\n",
      " 75.02214645 67.81769155 72.35897325 80.13978265 63.38800405 52.32639335\n",
      " 64.79514335 61.30384515 55.31165765 61.04043935 83.03777145 78.45361255\n",
      " 51.45450185 60.10853265 65.73486255 67.04043935 34.09290765 87.60027145\n",
      " 67.99356435 27.23442196 85.16455005 66.46790765 57.74045585 52.32639335\n",
      " 54.38800405 85.56902145 61.61367725 70.34112905 31.12237905 55.18798755\n",
      " 65.80117725 70.59805225 81.49089645 75.33642505 79.97882866 51.36634515\n",
      " 49.34894155 75.25519155 81.95139335 86.22012685 73.81902145 51.88019155\n",
      " 80.26522325 65.30987905 73.20272325 58.12930225 70.31991075 78.35116075\n",
      " 20.7552831  91.78084825 74.27951835 55.01211475 72.24178575 68.17147325\n",
      " 64.08420585 91.78084825 77.99178575 65.99826835 61.08553575 61.18017505\n",
      " 70.55295585 38.97793935 33.34770321 44.83429741 73.08420585 90.71745895\n",
      " 50.63330005 68.77081655 89.52862905 73.81902145 66.60764335 76.70183395\n",
      " 54.13241075 52.56858085 47.77170585 52.75226616 89.88800405 45.85764335\n",
      " 47.71320366 97.63330005 58.59894155 69.07817196 70.20920585 71.73486255\n",
      " 61.94402145 70.67928575 52.56858085 61.37148975 25.46701835 56.77951835\n",
      " 83.26389335 74.41277145 67.20272325 48.83509515 85.59023975 86.02303575\n",
      " 62.85293935 77.17706655 63.67414116 72.69580005 70.10675405 74.50741075\n",
      " 87.71701835 94.57461475 69.59983085 50.63330005 52.64981435 64.26007866\n",
      " 53.87548755 70.89759515 60.18195366 65.64022325 73.18665765 77.24356435\n",
      " 62.00785946 70.62726616 78.10897325 76.45050405 58.70964645 87.10764335\n",
      " 76.79647325 45.62326835 76.55295585 78.37237905 68.42706655 72.10205005\n",
      " 49.59894155 93.44580005 51.27081655 64.84023975 31.98086475 67.57550405\n",
      " 75.34290765 69.71053575 59.03173755 69.83420585 76.48664116 73.45183395\n",
      " 66.83553575 74.63108085 65.72793935 80.63241075 90.22616075 65.87459825\n",
      " 47.15673755 53.29514335 35.0646746  57.23353265 52.92236255 44.99915765\n",
      " 58.26789116 63.87930225 68.77081655 66.49178575 72.24178575 57.12326835\n",
      " 69.24826835 47.88197015 75.24826835 65.97705005 66.71009515 71.55117725\n",
      " 67.13507866 85.28040765 64.94269155 77.17479585]\n",
      "Test set score: 0.85\n",
      "\n",
      "\n",
      "Test set predictions for original data set:\n",
      "[ 62.  87.  40.  77.  69.  78.  67.  46.  99.  46.  54.  53.  91.  84.\n",
      "  48.  33.  57.  81.  50.  80.  63.  52.  73.  66.  55.  68.  65.  60.\n",
      "  53.  69.  71.  93.  82.  48.  52.  92.  92.  69.  57.  69.  76.  62.\n",
      "  71.  57.  71.  52.  77.  59.  77.  44.  47.  67.  44.  61.  39.  63.\n",
      "  47.  69.  81.  56.  68.  74.  42.  69.  63.  59.  76.  47.  60.  61.\n",
      "  81.  90.  80.  58.  64.  71.  73.  70.  29.  73.  79.  79.  64.  79.\n",
      "  70.  77.  63.  94.  59.  43.  65.  54.  54.  73.  86.  70.  63.  52.\n",
      "  61.  70.  32.  90.  67.  34.  83.  67.  63.  52.  45.  87.  63.  73.\n",
      "  26.  54.  62.  67.  78.  79.  79.  54.  46.  67.  88.  82.  79.  40.\n",
      "  74.  70.  77.  61.  68.  67.  30.  81.  77.  61.  67.  64.  62.  81.\n",
      "  68.  60.  71.  68.  65.  41.  38.  48.  71.  97.  58.  69.  88.  70.\n",
      "  64.  86.  53.  52.  53.  59.  81.  40.  53.  97.  65.  71.  67.  81.\n",
      "  66.  68.  54.  57.  22.  56.  83.  68.  73.  50.  93.  88.  76.  77.\n",
      "  72.  71.  67.  72.  96. 100.  62.  59.  58.  64.  50.  71.  58.  63.\n",
      "  74.  84.  65.  62.  77.  74.  62.  91.  61.  35.  72.  74.  66.  72.\n",
      "  51.  94.  50.  59.  32.  55.  61.  62.  55.  65.  75.  68.  72.  63.\n",
      "  65.  78.  84.  69.  46.  62.  36.  53.  58.  42.  62.  62.  67.  77.\n",
      "  71.  46.  66.  49.  67.  59.  65.  71.  63.  90.  55.  79.]\n",
      "Test set score: 1.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "diffDataSets = [topTwo, topFour, topSix, data]\n",
    "dataSetNames = [\"topTwo\", \"topFour\", \"topSix\", \"original data set\"]      # This is for easier output interpretation\n",
    "target = data[\"math score\"]\n",
    "# d = data.drop(columns=[\"math score\"])   # Remove the target column from the data\n",
    "\n",
    "count = 0\n",
    "for x in diffDataSets:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, target, random_state=0)  # split the data in to test and train\n",
    "\n",
    "    lr = LinearRegression().fit(x_train, y_train)   # Create a linear regression classifier and fit the train data into the model\n",
    "    y_predlr = lr.predict(x_test)   # predict the math score on the test data\n",
    "\n",
    "    print(\"Test set predictions for {}:\\n{}\".format(dataSetNames[count], y_predlr)) # print out the predictions for the test data \n",
    "\n",
    "    print(\"Test set score: {:.2f}\\n\\n\".format(lr.score(x_test, y_test))) # evaluate the accuracy  of your classifier \n",
    "    # the score function computes the success \n",
    "    count = count + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
